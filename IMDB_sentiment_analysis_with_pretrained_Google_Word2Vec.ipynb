{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Sentiment analysis with google Word2vec\n",
    "In this notebook, we will implement a neural network to do sentiment analysis on the IMDB movie comments using a pre-trained Word2Vec. We will also use a generator in order to spare some memory during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cthew\\anaconda3\\envs\\movie_reviews_3.6\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "INFO:nlpia.loaders:No BIGDATA index found in C:\\Users\\cthew\\anaconda3\\envs\\movie_reviews_3.6\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy C:\\Users\\cthew\\anaconda3\\envs\\movie_reviews_3.6\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to C:\\Users\\cthew\\anaconda3\\envs\\movie_reviews_3.6\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\cthew\\\\anaconda3\\\\envs\\\\movie_reviews_3.6\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\cthew\\\\anaconda3\\\\envs\\\\movie_reviews_3.6\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.loaders import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_urls_neg = pd.read_csv('./data/aclImdb/train/urls_neg.txt', sep='\\t',names=[\"imdb_urls\"])\n",
    "train_urls_pos = pd.read_csv('./data/aclImdb/train/urls_pos.txt', sep='\\t',names=[\"imdb_urls\"])\n",
    "test_urls_neg = pd.read_csv('./data/aclImdb/test/urls_neg.txt', sep='\\t',names=[\"imdb_urls\"])\n",
    "test_urls_pos = pd.read_csv('./data/aclImdb/test/urls_pos.txt', sep='\\t',names=[\"imdb_urls\"])\n",
    "name_basics_iterator = pd.read_csv('./data/name_basics/data.tsv', sep='\\t',chunksize=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell handles the recuperation of the file who contains the comment. We will also link the comment to the movies in case we need any information later about the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_id_files = '^\\d+'\n",
    "regex_id_titles = 't{2}\\d+'\n",
    "directory_data=[\"./data/aclImdb/train/\",\"./data/aclImdb/test/\"]\n",
    "list_title_comment_eval_train=[]\n",
    "list_title_comment_eval_test=[]\n",
    "directory_data_split=['neg','pos']\n",
    "#Retrieves the data scattered into the differents directories. \n",
    "for dir_data_path in directory_data:\n",
    "    for name_directory in directory_data_split:\n",
    "        with os.scandir(dir_data_path+name_directory) as files:\n",
    "            for file in files:\n",
    "                try:\n",
    "                    id_file = re.search(regex_id_files, file.name).group()\n",
    "                    movie_url=train_urls_neg.iloc[int(id_file)].imdb_urls\n",
    "                    id_title = re.search(regex_id_titles, movie_url).group()\n",
    "                    f = open(file, \"r\", encoding=\"utf8\")\n",
    "                    review = f.read()\n",
    "                    if \"train\" in dir_data_path:\n",
    "                        list_title_comment_eval_train.append([id_title,name_directory,review])\n",
    "                    else:\n",
    "                        list_title_comment_eval_test.append([id_title,name_directory,review])\n",
    "                except AttributeError:\n",
    "                    print(\"Error regex should have been found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame(list_title_comment_eval_train, columns=[\"id_title\", \"sentiment\", \"reviews\"])\n",
    "test_df=pd.DataFrame(list_title_comment_eval_test, columns=[\"id_title\", \"sentiment\", \"reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./dataset'):\n",
    "    os.makedirs('./dataset')\n",
    "train_df.to_pickle(\"./dataset/train_df.pkl\")\n",
    "test_df.to_pickle(\"./dataset/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset \n",
    "train_df=pd.read_pickle(\"./dataset/train_df.pkl\")\n",
    "test_df=pd.read_pickle(\"./dataset/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the row of our dataset. \n",
    "#Our target his balanced and is equally represented in the test and in the train\n",
    "dataset_df=[shuffle(train_df),shuffle(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we change the categorical varaibles into numerical (Dummy)\n",
    "for dataframe in dataset_df:\n",
    "    dataframe[\"sentiment\"]=dataframe[\"sentiment\"].replace({'neg':0,'pos':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tokenize and vectorize the data. We will use the Google New pretrained word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ./GoogleNews-vectors-negative300.bin\n",
      "INFO:gensim.models.utils_any2vec:loaded (100000, 300) matrix from ./GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "#Unpacking the vectors with gensim\n",
    "word_vectors = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "dataset_size=train_df.shape[0]\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "steps_per_epoch = dataset_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset, chunk_size = batch_size):\n",
    "    \"\"\" \n",
    "    We use a function who yields generators to spare some memory during execution.  \n",
    "    All the step for the preprocessing has been gathered into this function.\n",
    "    So the first step is to tokenized and vectorized. The second is to give to each vector the same dimension. \n",
    "    ------\n",
    "    dataset : (PandasDataFrame) the imdb data which need to be preprocess \n",
    "\n",
    "    chunk_size : Size of the yields batch who would be given to the model. \n",
    "                 The default value is set by the variable batch_size\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    batch_x : (Generator) the result of preprocessing on reviews, inputs used to do the prediction. \n",
    "    batch_y : (Generator) the target of the training\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    num_chunks = len(dataset) // chunk_size + 1\n",
    "    zero_vector = []\n",
    "    # This would be the vector that we had if the review is too short\n",
    "    # The value of 300 has been defined by the average length of a review tokenized\n",
    "    for _ in range(300):\n",
    "        zero_vector.append(0.0)\n",
    "    while True: #Loop forever so the generator never terminates\n",
    "        for i in range(num_chunks):\n",
    "            if dataset[i*chunk_size:(i+1)*chunk_size].shape[0]!=0:\n",
    "                batch_x = []\n",
    "                batch_y = []\n",
    "                for index, row in dataset[i*chunk_size:(i+1)*chunk_size].iterrows():\n",
    "                    tokens = tokenizer.tokenize(row[\"reviews\"])\n",
    "                    word_vectors_list = []\n",
    "                    #tokenize the data and then create a list of the vectors for those tokens\n",
    "                    for token in tokens:\n",
    "                        try:\n",
    "                            word_vectors_list.append(word_vectors[token].tolist())\n",
    "                        except KeyError:\n",
    "                            pass \n",
    "                    #Each input to a convolutional network must be equal in dimension.\n",
    "                    #So we have to define a maximum review length by maxlen.\n",
    "                    #You truncate any samples that is longer than maxlen and fill the one that are shorter with 0.\n",
    "                    if len(word_vectors_list) > maxlen:\n",
    "                        tmp = word_vectors_list[:maxlen]\n",
    "                    elif len(word_vectors_list) < maxlen:\n",
    "                        tmp = word_vectors_list\n",
    "                        additional_elems = maxlen - len(word_vectors_list)\n",
    "                        for _ in range(additional_elems):\n",
    "                            tmp.append(zero_vector)\n",
    "                    else:\n",
    "                        tmp = word_vectors_list\n",
    "                    batch_x.append(tmp)\n",
    "                    batch_y.append(row[\"sentiment\"])\n",
    "                batch_x = np.reshape(batch_x, (len(batch_x), maxlen, embedding_dims))   \n",
    "                batch_y = np.array(batch_y) \n",
    "                yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_dataset=[]\n",
    "for dataframe in dataset_df:\n",
    "    vectorize_dataset.append(preprocessing(dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=vectorize_dataset[0]\n",
    "test_generator=vectorize_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(\n",
    "    250,\n",
    "    3,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    input_shape=(maxlen, embedding_dims)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "781/781 [==============================] - 347s 444ms/step - loss: 0.4701 - accuracy: 0.7604 - val_loss: 0.3566 - val_accuracy: 0.8433\n",
      "Epoch 2/2\n",
      "781/781 [==============================] - 345s 443ms/step - loss: 0.2670 - accuracy: 0.8889 - val_loss: 0.3543 - val_accuracy: 0.8554\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_generator, verbose=1, steps_per_epoch=steps_per_epoch,epochs=2, validation_data=test_generator,\n",
    "validation_steps=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model_structure = model.to_json()\n",
    "with open(\"IMDB_1D_CNN.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weights\n",
    "model.save_weights(\"IMDB_1D_CNN_Weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_reviews_3.6",
   "language": "python",
   "name": "movie_reviews_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
